{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a Recommendation System for Purchase Data\n",
    "\n",
    "The scope of this notebook is \n",
    "\n",
    "- Code the Scoring Function\n",
    "- Unit Test the Score\n",
    "- Build the Dash Applimcation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import sqlalchemy as sql\n",
    "\n",
    "#Data Science\n",
    "import pandas as pd\n",
    "\n",
    "#Model Tracking\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import ResponseError\n",
    "\n",
    "from surprise import dump\n",
    "\n",
    "#Utils\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import configparser\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "#Settings\n",
    "from pprint import pprint\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment variables\n",
    "outmodels = '../models/'\n",
    "artefact_name = '4fa76aa5e00c413db3e23810a913dc8e'\n",
    "\n",
    "# Set dbconnection variables\n",
    "dbconnPath = './dbconn.properties'\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(dbconnPath)\n",
    "params = config\n",
    "db_host=params.get('CONN', 'host')\n",
    "db_port=params.get('CONN', 'port')\n",
    "db_user=params.get('CONN', 'user')\n",
    "db_pwd=params.get('CONN', 'password')\n",
    "db_name=params.get('CONN', 'database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model Artefact from the Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "for regmodel in client.list_registered_models():\n",
    "    regmodel_info = dict(regmodel)\n",
    "\n",
    "# pprint(regmodel_info, indent=3)\n",
    "\n",
    "champion=client.get_registered_model('Champion')\n",
    "championid=champion.latest_versions[-1].run_id\n",
    "\n",
    "art_list = [arts.path for arts in client.list_artifacts(championid, path=None)]\n",
    "\n",
    "for art_path in art_list: \n",
    "    client.download_artifacts(championid, art_path, outmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Check Model Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prediction(uid='10929', iid='261', r_ui=0.0, est=0.3747662865053879, details={'was_impossible': False}), Prediction(uid='2715', iid='256', r_ui=0.0, est=1.397448561679778, details={'was_impossible': False}), Prediction(uid='12786', iid='198', r_ui=0.0, est=0.34800521979059373, details={'was_impossible': False}), Prediction(uid='38', iid='119', r_ui=0.0, est=1.4081730858161146, details={'was_impossible': False}), Prediction(uid='8417', iid='2', r_ui=0.0, est=0.10200631019335304, details={'was_impossible': False}), Prediction(uid='5370', iid='293', r_ui=0.0, est=1.4426131167097673, details={'was_impossible': False}), Prediction(uid='225', iid='34', r_ui=0.0, est=0.6310815188817669, details={'was_impossible': False}), Prediction(uid='12930', iid='24', r_ui=2.0, est=0.5743177052276057, details={'was_impossible': False}), Prediction(uid='9481', iid='261', r_ui=0.0, est=0.45208051960508466, details={'was_impossible': False}), Prediction(uid='2410', iid='49', r_ui=0.0, est=0.7374420578118797, details={'was_impossible': False})]\n",
      "****************************************************************************************************\n",
      "<surprise.prediction_algorithms.baseline_only.BaselineOnly object at 0x7f4c8da5e5d0>\n"
     ]
    }
   ],
   "source": [
    "modelpkl = [modelpath for modelpath in glob.glob(outmodels + 'model/*.pkl')][0]\n",
    "modelpkl\n",
    "\n",
    "predictions, algo = dump.load(modelpkl)\n",
    "\n",
    "print(predictions[0:10])\n",
    "print('*'*100)\n",
    "print(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test One: Score the single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10089</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10171</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0     100       0       1\n",
       "1    1007       0       1\n",
       "2   10089       0       0\n",
       "3    1011       0       0\n",
       "4   10171       0       9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = pd.read_csv(''.join([outmodels, 'sample2scoredo9qo832.csv']))\n",
    "test_sample\n",
    "\n",
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1007       item: 0          r_ui = 1.00   est = 0.86   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "uid = str(1007)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(0)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo.predict(uid, iid, r_ui=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Two: Score the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(100, 0, 1, 0.6463847965824251, {'was_impossib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>(1007, 0, 1, 0.6463847965824251, {'was_impossi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10089</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(10089, 0, 0, 0.6463847965824251, {'was_imposs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(1011, 0, 0, 0.6463847965824251, {'was_impossi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10171</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>(10171, 0, 9, 0.6463847965824251, {'was_imposs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating                                        predictions\n",
       "0     100       0       1  (100, 0, 1, 0.6463847965824251, {'was_impossib...\n",
       "1    1007       0       1  (1007, 0, 1, 0.6463847965824251, {'was_impossi...\n",
       "2   10089       0       0  (10089, 0, 0, 0.6463847965824251, {'was_imposs...\n",
       "3    1011       0       0  (1011, 0, 0, 0.6463847965824251, {'was_impossi...\n",
       "4   10171       0       9  (10171, 0, 9, 0.6463847965824251, {'was_imposs..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample['predictions'] = test_sample.apply(lambda row:algo.predict(row['userID'], \n",
    "                     row['itemID'], row['rating']), axis = 1)\n",
    "test_sample.head()\n",
    "# predictions=list(data_prep_5['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "We have an mobile app allowing its customers to place orders before they even have to walk into the store.\n",
    "When a customer first taps on the “order” page, we may recommend \n",
    "\n",
    "1. Top items to be added to their basket, e.g. disposable utensils, fresh meat, chips, and and so on.\n",
    "2. Personalized recommendation with ranked list of items (product IDs) that the user is most likely to want to put in his/her (empty) “basket”\n",
    "\n",
    "Assuming that the scenario is ModelOps 0. Then: \n",
    "\n",
    "1. Data scientists hand over a trained model as an artifact to the engineering team for deployement\n",
    "2. The handoff can include putting the trained model in the models registry\n",
    "3. The Scoring process is in Batch on a sigle EC2 instance\n",
    "\n",
    "We have to reproduce the required development enviroment\n",
    "\n",
    "0. Define Artefacter function to get the last version of Champion Model (optional)\n",
    "\n",
    "1. Define Scoring Functions: Batch scoring is the main assumption\n",
    "\n",
    "    - Define the get_top_items function \n",
    "    - Define the get_top_n_ui function\n",
    "    \n",
    "\n",
    "2. Unit Test \n",
    "\n",
    "3. Define a quick front end that simulate Mobile App (Test it in Docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top(predictions):\n",
    "    \n",
    "    '''\n",
    "    Returns the the top-N recommendation from a set of predictions\n",
    "    \n",
    "    '''\n",
    "    top_n = defaultdict(list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Recommended Items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    print(top_n)\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_ui(top, ui):\n",
    "    try:\n",
    "        return {k:v for k,v in top.items() if ui==k}\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set connection string\n",
    "connection_str = f'mysql+pymysql://{db_user}:{db_pwd}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# connect to database\n",
    "engine = sql.create_engine(connection_str)\n",
    "connection = engine.connect()\n",
    "test_sample = pd.read_sql(\"select * from CUSTOMERID\", connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, row in test_sample[['customerId']].iterrows():\n",
    "#     print(row)\n",
    "#     get_top_n_ui(get_top_n(predictions, n=10), row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
