{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Recommendation System for Purchase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#Data\n",
    "import sqlalchemy as sql\n",
    "\n",
    "#Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor, BaselineOnly\n",
    "from surprise import KNNBasic,KNNWithMeans,KNNWithZScore,KNNBaseline\n",
    "from surprise import SVD,SVDpp,NMF\n",
    "from surprise import SlopeOne, CoClustering\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.accuracy import rmse, mae\n",
    "from surprise import dump\n",
    "\n",
    "#Model Tracking\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "# TRACKING_URI = 'http://mlflow:5000'\n",
    "# mlflow.set_tracking_uri(TRACKING_URI)\n",
    "\n",
    "#Utils\n",
    "import os\n",
    "import configparser\n",
    "from collections import defaultdict\n",
    "import tempfile\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment Variables\n",
    "outdata = '../data'\n",
    "outmodels = '../models'\n",
    "dbconnPath = './dbconn.properties'\n",
    "\n",
    "# Set dbconnection variables\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(dbconnPath)\n",
    "params = config\n",
    "db_host=params.get('CONN', 'host')\n",
    "db_port=params.get('CONN', 'port')\n",
    "db_user=params.get('CONN', 'user')\n",
    "db_pwd=params.get('CONN', 'password')\n",
    "db_name=params.get('CONN', 'database')\n",
    "\n",
    "# Set connection string\n",
    "connection_str = f'mysql+pymysql://{db_user}:{db_pwd}@{db_host}:{db_port}/{db_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "We have to:\n",
    "\n",
    "1) Split each list of items in the products column into rows\n",
    "\n",
    "2) Count the number of products bought by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "engine = sql.create_engine(connection_str)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database contains CUSTOMERID, TRANSACTIONS tables\n",
      "database contains CUSTOMERID table\n"
     ]
    }
   ],
   "source": [
    "print(f'{db_name} contains {engine.table_names()[0]}, {engine.table_names()[1]} tables')\n",
    "print(f'{db_name} contains {engine.table_names()[0]} table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tables in dataframes (Don't do that if you have a large dataset. Use just samples)\n",
    "df_cus = pd.read_sql(\"select * from CUSTOMERID\", connection)\n",
    "df_trx = pd.read_sql(\"select * from TRANSACTIONS\", connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>27773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id customerId\n",
       "0   1       1553\n",
       "1   2      20400\n",
       "2   3      19750\n",
       "3   4       6334\n",
       "4   5      27773"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_cus.shape)\n",
    "df_cus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62483, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customerId</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2|2|23|68|68|111|29|86|107|152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>111|107|29|11|11|11|33|23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>164|227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2|2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id customerId                        products\n",
       "0   1          0                              20\n",
       "1   2          1  2|2|23|68|68|111|29|86|107|152\n",
       "2   3          2       111|107|29|11|11|11|33|23\n",
       "3   4          3                         164|227\n",
       "4   5          5                             2|2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_trx.shape)\n",
    "df_trx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_1=pd.DataFrame(df_trx.products.str.split('|').tolist(), index=df_trx.customerId)\\\n",
    ".stack()\\\n",
    ".reset_index()\\\n",
    ".groupby(['customerId', 0])\\\n",
    ".agg({0: 'count'})\\\n",
    ".rename(columns={0: 'purchase_count'})\\\n",
    ".reset_index()\\\n",
    ".rename(columns={0: 'productId'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customerId productId  purchase_count\n",
      "0          0         1               2\n",
      "1          0        13               1\n",
      "2          0       136               2\n",
      "3          0       157               1\n",
      "4          0        19               3\n",
      "****************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 133585 entries, 0 to 133584\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   customerId      133585 non-null  object\n",
      " 1   productId       133585 non-null  object\n",
      " 2   purchase_count  133585 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_prep_1.head())\n",
    "print('*'*40)\n",
    "print(data_prep_1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's normalize data in a way to have a rank\n",
    "data_prep_2 = pd.pivot_table(data=data_prep_1, index='customerId', columns='productId', values='purchase_count', aggfunc='sum')\n",
    "data_prep_3 = (data_prep_2 - data_prep_2.min())/(data_prep_2.max() - data_prep_2.min())\n",
    "data_prep_4 = data_prep_3.reset_index().melt(id_vars=['customerId'], value_name='prod_ratings').dropna()\n",
    "data_prep_4.index = np.arange(0, len(data_prep_4))\n",
    "data_prep_4['prod_ratings'] = data_prep_4['prod_ratings'].apply(lambda x: int((round(x, 2))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       customerId productId  prod_ratings\n",
      "1307            0         1            10\n",
      "54157           0        20             0\n",
      "49722           0       198             0\n",
      "46478           0        19            14\n",
      "116292          0        69            20\n"
     ]
    }
   ],
   "source": [
    "print(data_prep_4.sort_values(by=['customerId']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get **Triplet Representation** with each column representing user, item and the given rating respectively.\n",
    "Notice the rating goes from 0–100 (with 100 being the most number of purchase for an item and 0 being 0 purchase count for that item).\n",
    "It's a kind of preference for each user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt1=data_prep_4['prod_ratings']\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(df_plt1)\n",
    "ax.set_xlabel('Syntetic Product Ratings')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Rating Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Distribution by ProductId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_2=data_prep_4.groupby('productId')['prod_ratings'].count().reset_index()[0:50]\n",
    "df_plt_2\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.bar(df_plt_2.productId, df_plt_2.prod_ratings, )\n",
    "ax.set_xlabel('Number of Ratings per product')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by ProductId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_2.sort_values(by='prod_ratings', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_2=data_prep_4.groupby('productId')['prod_ratings'].count()\n",
    "df_plt_2\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(df_plt_2)\n",
    "ax.set_xlabel('Number of Ratings')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by ProductId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratings Distribution By User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_3=data_prep_4.groupby('customerId')['prod_ratings'].count().reset_index()[0:50]\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.bar(df_plt_3.customerId, df_plt_3.prod_ratings)\n",
    "ax.set_xlabel('Number of Ratings per Customer')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by customerId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_3.sort_values(by='prod_ratings', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_3=data_prep_4.groupby('customerId')['prod_ratings'].count()\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(df_plt_3)\n",
    "ax.set_xlabel('Number of Ratings per Customer')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Rating Distribution by customerId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "To do that, I use **Surprise** which is an easy-to-use Python scikit for recommender systems.\n",
    "\n",
    "I follow the Getting Started (https://surprise.readthedocs.io/en/stable/getting_started.html#getting-started)\n",
    "\n",
    "Reference: https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Utility Functions (mlflow_tracker, get_Iu, get_Ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "BucketAlreadyOwnedByYou",
     "evalue": "BucketAlreadyOwnedByYou: message: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/work/ModelOps/notebooks/00_set_bucket.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mminioClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mlflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mResponseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/minio/api.py\u001b[0m in \u001b[0;36mmake_bucket\u001b[0;34m(self, bucket_name, location)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_bucket_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m: BucketAlreadyOwnedByYou: message: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "# A reader is required with the rating_scale param\n",
    "mindata = data_prep_4.prod_ratings.min()\n",
    "maxdata = data_prep_4.prod_ratings.max()\n",
    "reader = Reader(rating_scale=(mindata,maxdata))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order)\n",
    "data_prep_5 = data_prep_4.rename(columns={'customerId': 'userID', 'productId':'itemID', 'prod_ratings':'rating'})\n",
    "data_prep_5.to_csv(os.path.join(outdata, 'sample_toscore.csv'))\n",
    "data = Dataset.load_from_df(data_prep_5[['userID', 'itemID', 'rating']], reader)\n",
    "\n",
    "#Create the experiment\n",
    "mlflow.set_experiment('Purchase Recommended System')\n",
    "\n",
    "#Create Minio Bucket for Artefact Storage\n",
    "%run -t 00_set_bucket.ipynb\n",
    "\n",
    "def mlflow_tracker(reader_data, algo, params, rundesc='myrun'):\n",
    "        \n",
    "    # Store Algo name\n",
    "    algo_name = str(algo.__class__.__name__)\n",
    "\n",
    "    with mlflow.start_run(run_name=algo_name) as run:\n",
    "\n",
    "        # Store run_id and experiment_id\n",
    "        run_id=run.info.run_uuid\n",
    "        experiment_id=run.info.experiment_id\n",
    "        \n",
    "        #Create model instance\n",
    "        redic=cross_validate(algo, reader_data, **params)\n",
    "        #Create a dataframe of means\n",
    "        recdf=pd.DataFrame.from_dict(redic).mean(axis=0)\n",
    "\n",
    "        #Log params\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric('test_time', recdf.iloc[3])\n",
    "        mlflow.log_metric('test_rmse_mean', recdf.iloc[0])\n",
    "        mlflow.log_metric('test_mae_mean', recdf.iloc[1])\n",
    "        mlflow.log_metric('fit_time', recdf.iloc[2])\n",
    "\n",
    "        # Set the notes for Runs\n",
    "\n",
    "        MlflowClient().set_tag(run_id,\n",
    "                               \"mlflow.note.content\",\n",
    "                               rundesc)\n",
    "\n",
    "    return (run_id, experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage: Automatic cross-validation\n",
    "\n",
    "#### Basic Algorithms\n",
    "\n",
    "##### random_pred.NormalPredictor\n",
    "Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n",
    "##### baseline_only.BaselineOnly\n",
    "Algorithm predicting the baseline estimate for given user and item.\n",
    "\n",
    "#### KNN algorithms\n",
    "\n",
    "##### knns.KNNBasic\n",
    "A basic collaborative filtering algorithm\n",
    "##### knns.KNNWithMeans\n",
    "A basic collaborative filtering algorithm, taking into account the mean ratings of each user.\n",
    "##### knns.KNNWithZScore\n",
    "A basic collaborative filtering algorithm, taking into account taking into account the z-score normalization of each user.\n",
    "##### knns.KNNBaseline\n",
    "A basic collaborative filtering algorithm taking into account a baseline rating.\n",
    "\n",
    "#### Matrix Factorization-based algorithms\n",
    "##### matrix_factorization.SVD\n",
    "The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.When baselines are not used, this is equivalent to Probabilistic Matrix Factorization\n",
    "##### matrix_factorization.SVDpp\n",
    "The SVD++ algorithm, an extension of SVD taking into account implicit ratings.\n",
    "##### matrix_factorization.NMF\n",
    "A collaborative filtering algorithm based on Non-negative Matrix Factorization.\n",
    "\n",
    "#### Other Algorithms\n",
    "##### slope_one.SlopeOne\n",
    "A simple yet accurate collaborative filtering algorithm.\n",
    "##### co_clustering.CoClustering\n",
    "A collaborative filtering algorithm based on co-clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'measures': ['RMSE', 'MAE'], 'cv':3, 'verbose': False}\n",
    "# algos = [BaselineOnly(), NormalPredictor(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), KNNBaseline(), SVD(), SVDpp(), SlopeOne(), CoClustering()]\n",
    "\n",
    "algos = [BaselineOnly()]\n",
    "\n",
    "for algo in algos:\n",
    "    mlflow_tracker(data, algo, params)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:  Based on the Mlflow tracking service, the best model is BaselineOnly algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparer(data):\n",
    "    trainset, testset = train_test_split(data, test_size=0.25)\n",
    "    return trainset, testset\n",
    "\n",
    "def trainer(trainset, bsl_options):\n",
    "    algo = BaselineOnly(bsl_options=bsl_options)\n",
    "    model = algo.fit(trainset)\n",
    "    return model\n",
    "\n",
    "def predictor(model, testset):\n",
    "    predictions = model.test(testset)\n",
    "    return model, predictions\n",
    "\n",
    "def tuner(data, bsl_options_grid, param_model):\n",
    "    \n",
    "    gs = GridSearchCV(BaselineOnly, bsl_options_grid, **param_model)\n",
    "    gs.fit(data)\n",
    "    \n",
    "    history_tune=pd.DataFrame.from_dict(gs.cv_results)\n",
    "    \n",
    "    best_bsl_options = gs.best_params['rmse']\n",
    "    \n",
    "    train, test = preparer(data)\n",
    "    tuned_model = trainer(train, best_bsl_options)\n",
    "    \n",
    "    return param_model, history_tune, tuned_model\n",
    "\n",
    " # Define utils functions for prediction readability\n",
    "def get_Iu(uid):\n",
    "    \"\"\" return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "\n",
    "def get_Ui(iid):\n",
    "    \"\"\" return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def mlflow_tune_tracker(data, algo_name, param_grid, param_model):\n",
    "\n",
    "    with mlflow.start_run(run_name=algo_name) as run:\n",
    "        \n",
    "        # Store run_id and experiment_id\n",
    "        run_id=run.info.run_uuid\n",
    "        experiment_id=run.info.experiment_id\n",
    "        \n",
    "        #Train the model\n",
    "        trainset, testset = preparer(data)\n",
    "        \n",
    "        #Tune\n",
    "        params, history_tune, tuned_model = tuner(data, param_grid, param_model)\n",
    "\n",
    "        #History\n",
    "        for index, row in history_tune.iterrows():\n",
    "            with mlflow.start_run(experiment_id=experiment_id, run_name=algo_name + str(index), nested=True) as subruns:\n",
    "                \n",
    "                bsl_options = row['params']\n",
    "                params_tune = {**params, **bsl_options}\n",
    "\n",
    "                #Log params\n",
    "                mlflow.log_params(params_tune)\n",
    "                mlflow.log_metric('fit_time',round(row['mean_fit_time'], 3))\n",
    "                mlflow.log_metric('test_time', round(row['mean_test_time'], 3))\n",
    "                mlflow.log_metric('test_rmse_mean', round(row['mean_test_rmse'], 3))\n",
    "                mlflow.log_metric('test_mae_mean', round(row['mean_test_mae'], 3))\n",
    "                \n",
    "                #Log Model (artefact)\n",
    "                temp = tempfile.NamedTemporaryFile(prefix=\"model\", suffix=\".pkl\")\n",
    "                temp_name = temp.name\n",
    "                try:\n",
    "                    model, predictions = predictor(trainer(trainset, bsl_options), testset)\n",
    "                    dump.dump(temp_name, predictions, model)\n",
    "                    mlflow.log_artifact(temp_name, 'model')\n",
    "                finally:\n",
    "                    temp.close()\n",
    "                    \n",
    "                 #Log best predictions\n",
    "                df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "                df['Iu'] = df.uid.apply(get_Iu(trainset))\n",
    "                df['Ui'] = df.iid.apply(get_Ui(trainset))\n",
    "                df['err'] = abs(df.est - df.rui)\n",
    "                best_predictions = df.sort_values(by='err')[:10]\n",
    "                temp = tempfile.NamedTemporaryFile(prefix=\"best-predicitions\", suffix=\".csv\")\n",
    "                temp_name = temp.name\n",
    "                try:\n",
    "                    best_predictions.to_csv(temp_name, index=False)\n",
    "                    mlflow.log_artifact(temp_name)\n",
    "                finally:\n",
    "                    temp.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_Iu() missing 1 required positional argument: 'uid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-62fd61f8330c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# mlflow_tune_tracker(params, history_tune, tuned_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmlflow_tune_tracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BaselineOnly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-43525a3699e7>\u001b[0m in \u001b[0;36mmlflow_tune_tracker\u001b[0;34m(data, algo_name, param_grid, param_model)\u001b[0m\n\u001b[1;32m     91\u001b[0m                  \u001b[0;31m#Log best predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rui'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'details'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Iu'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_Iu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ui'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_Ui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'err'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mest\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_Iu() missing 1 required positional argument: 'uid'"
     ]
    }
   ],
   "source": [
    "param_grid = {'bsl_options' : {'method': ['als', 'sgd'],\n",
    "              'n_epochs': [5, 15],\n",
    "              'reg_u': [10, 20],\n",
    "              'reg_i': [5,15]               \n",
    "                }\n",
    "              }\n",
    "\n",
    "param_model = {\n",
    "          'measures': ['RMSE', 'MAE'], \n",
    "          'cv':3\n",
    "            }\n",
    "\n",
    "# params, history_tune, tuned_model = tuner(data, param_grid, param_model)\n",
    "\n",
    "# mlflow_tune_tracker(params, history_tune, tuned_model)\n",
    "\n",
    "mlflow_tune_tracker(data, 'BaselineOnly', param_grid, param_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_tune_tracker(tuned_model, params_tune, history_tune):\n",
    "    \n",
    "            trainset, testset = train_test_split(reader_data, test_size=testsize)\n",
    "            algo_best = BaselineOnly(bsl_options=model_params)\n",
    "            predictions = algo_best.fit(trainset).test(testset)\n",
    "            \n",
    "            #Log params\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metric('test_rmse_mean', round(rmse(predictions), 3))\n",
    "            mlflow.log_metric('test_mae_mean', round(mae(predictions), 3))\n",
    "            \n",
    "            #Log Model (artefact)\n",
    "            dump.dump(os.path.join(outputdir, 'model.pkl'), predictions, algo_best)\n",
    "            mlflow.log_artifact(os.path.join(outputdir, 'model.pkl'), 'model')\n",
    "            \n",
    "            #Log best predictions\n",
    "            df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "            df['Iu'] = df.uid.apply(get_Iu)\n",
    "            df['Ui'] = df.iid.apply(get_Ui)\n",
    "            df['err'] = abs(df.est - df.rui)\n",
    "            best_predictions = df.sort_values(by='err')[:10]\n",
    "            temp = tempfile.NamedTemporaryFile(prefix=\"best-predicitions\", suffix=\".csv\")\n",
    "            temp_name = temp.name\n",
    "            try:\n",
    "                best_predictions.to_csv(temp_name, index=False)\n",
    "                mlflow.log_artifact(temp_name)\n",
    "            finally:\n",
    "                temp.close() # Delete the temp file\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the best parameter combination**\n",
    "\n",
    "Then the best model is BaselineOnly with \n",
    "- method: 'als'\n",
    "- n_epochs: 10\n",
    "- reg_u: 12\n",
    "- reg_i: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "algo_best = BaselineOnly(bsl_options=bsl_options)\n",
    "predictions = algo_best.fit(trainset).test(testset)\n",
    "print(round(rmse(predictions), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, algo = dump.load(outmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = data_prep_4.head()\n",
    "test_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = str(1007)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(0)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo_best.predict(uid, iid, r_ui=13, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_5['predictions'] = data_prep_5.apply(lambda row:algo_best.predict(row['userID'], \n",
    "                     row['itemID'], row['rating']), axis = 1)\n",
    "data_prep_5.head()\n",
    "# predictions=list(data_prep_5['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Recommended Items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "#     # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def get_top_n_ui(top, ui):\n",
    "    try:\n",
    "        return {k:v for k,v in top.items() if ui==k}\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_top_n_ui(get_top_n(predictions, n=10), '20400'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model with the MLflow Model Registry API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id, experiment_id = mlflow_tracker (data, algo, 'Trained', bsl_options, params, testsize=0.25, outputdir=outmodels, runame='run1', expdesc='myexperiment', rundesc='myrun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Champion'\n",
    "artifact_path = \"model\"\n",
    "model_uri = \"runs:/{run_id}/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n",
    "\n",
    "model_details = mlflow.register_model(model_uri=model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "client.update_registered_model(\n",
    "  name=model_details.name,\n",
    "  description=\"This model provides recommendation for specific users and items based on purchase data. The data consists of user transactions\"\n",
    ")\n",
    "\n",
    "client.update_model_version(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    "  description=\"This model was built with Surprise library. It is a ALS based BaselineOnly algorithm\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
