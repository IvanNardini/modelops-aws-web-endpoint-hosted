# Configure AWS EC2

curl -fsSL https://get.docker.com -o get-docker.sh

sudo sh get-docker.sh

sudo usermod -aG docker ubuntu

sudo curl -L "https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose

sudo chmod +x /usr/local/bin/docker-compose

gitclone https://github.com/IvanNardini/ModelDeploy-AWS-HostedWebEndPoint.git

# data - MySQL service

The entrypoint script checks if DATADIR/mysql exists. 
If not, it runs --initialize, but if DATADIR/ contains any files other than 
ones starting with . or specified with --ignore-db-dir, --initialize will fail with that error message.
So the directory you map to :/var/lib/mysql should either contain an existing MySQL database or be completely empty

https://github.com/docker-library/mysql/issues/118
Also, the default config of the server from the apt repo is to only allow files from /var/lib/mysql-files/ to be loaded 
with LOAD DATA INFILE (mysql docs). It would become unwieldy if we had an environment variable for every mysql option. 
I would suggest either mounting your files in that directory and putting an import script into /docker-entrypoint-initdb.d/ 
to load it, or add a mysql config file to /etc/mysql/conf.d/ to change the secure-file-priv value to something more 
convenient.

https://mysqlserverteam.com/what-is-load-data-and-how-does-it-work/

https://dba.stackexchange.com/questions/249637/mysql-load-data-infile-error-1064

It was \n

To test:

docker build -t mysql-test .

docker container run --name testsql -p 8079:8079 mysql-dev-test:latest

docker exec -it mysql-test bin/bash

mysql -u root -p 
>>Orion123

show databases;
use database;
show tables;
SELECT * FROM TRANSACTIONS LIMIT 10;

Notice When you LOAD TABLE if you run the demo on Windows use \r\n else \n 

# notebook - Jupyter Service

SQLAlchemy uses the dialect which is the system to communicate with various 
types of DBAPI implementations and databases.
To specifically mention DB-API to be used for connection, the URL string takes the form as follows −

dialect[+driver]://user:password@host/dbname

For a MySQL database, use the below command −
engine = create_engine("mysql://user:pwd@localhost/college",echo = True)

But if you run conda install mysql-python, it doesn't work with this enviroment. Indeed...

Found conflicts! Looking for incompatible packages.
UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - mysql-python -> python[version='>=2.7,<2.8.0a0']

My python: python=3.7

Now MySQLAlchemy supports many dialect/DBAPI options. One of them is PyMySQL.

https://docs.sqlalchemy.org/en/13/dialects/mysql.html#module-sqlalchemy.dialects.mysql.pymysql

So conda install pymysql

Notice I tested MySQL-Connector but It does not work for me! 

# Model Tracking service

I just follow the blog below

https://towardsdatascience.com/containerize-your-whole-data-science-environment-or-anything-you-want-with-docker-compose-e962b8ce8ce5

# Tracking backend database

I had a problem on defining a new database enviroment. It depends from the volume (or bind mount directory)
being already initialized after your first start. The postgres user, and database creation only happens on the first start 
(ie, /var/lib/postgresql/data must not already contain database files).

Delete the image and the volume if you change the ENVs variables

# Tracking artefacts

Initially the error was 'permission denied create /mlflow'

So I modified the JupyterLab image

But then...

Can not log_artifact to remote server

MLflow doesn't currently support logging artifacts to the local filesystem of a remote tracking server. 
This is because artifacts are actually written by the client, which just obtains an artifact location 
to write to from the tracking server. Instead, we recommend launching your tracking server with 
--default-artifact-root set to an artifact location accessible to both the tracking server & the client, 
e.g. an S3 bucket URI. See the tracking docs and this SO post for more information.

Also one gotcha worth noting from the SO post:
Note that the server uses --default-artifact-root only when assigning artifact roots to newly-created experiments 
- runs created under existing experiments will use an artifact root directory under the existing experiment's artifact root.
So you'll need to create a new experiment for the artifact root change to take place. Thanks all for discussing this, it's come up a few times -
I'd suggest these simple changes to make using log_artifact smoother:
Log a warning when mlflow server is run without --default-artifact-root (and eventually, require --default-artifact-root)
Log the artifact path being used when log_artifact is called.

I decide to go with Minio

Notice I had some problem on setting different env variable in dockerfile

When you go to Minio you have to set all envrioment variable and add boto3 in MLflow server 

Warning: If you do not specify a --default-artifact-root, nor do you specify an artifact URI when creating the experiment (e.g., mlflow experiments create --artifact-root s3://), then the artifact root will be a path inside the File Store. Typically this is not an appropriate location, as the client and server will probably be referring to different physical locations (i.e., the same path on different disks).

The artifact store (used for log_model or log_artifact) is used to persist the larger data such as models, which is why we rely on an external persistent store. This is why the log_metric and log_param calls work -- they only need to talk to the server -- while the log_model call is failing.

#############################################################################################################################



